#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ü§ñ –¢—Ä–µ–Ω–µ—Ä –¥–ª—è GRU –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤
—Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π.
"""

import pickle
import time
from pathlib import Path

import numpy as np
import torch
from tqdm import tqdm

from ..utils.metrics import evaluate_anomaly_detection, print_metrics_report
from .trainer import Trainer


class AutoencoderTrainer(Trainer):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤."""

    def __init__(self, model, optimizer, criterion, device="cuda", log_dir="logs"):
        """
        Args:
            model: GRU –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (–æ–±—ã—á–Ω–æ CrossEntropy)
            device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            log_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤
        """
        # –ù–µ –≤—ã–∑—ã–≤–∞–µ–º super().__init__() —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –Ω—É–∂–Ω–∞ –¥—Ä—É–≥–∞—è –ª–æ–≥–∏–∫–∞
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = torch.device(
            device if torch.cuda.is_available() and device == "cuda" else "cpu"
        )

        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = self.model.to(self.device)

        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            "train_losses": [],
            "val_mean_errors": [],
            "val_std_errors": [],
            "val_median_errors": [],
            "val_samples": [],
            "epochs_with_validation": [],
        }

        print(f"ü§ñ –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Ç—Ä–µ–Ω–µ—Ä –≥–æ—Ç–æ–≤ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")

    def train_epoch(self, train_loader):
        """–û–±—É—á–∞–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –æ–¥–Ω—É —ç–ø–æ—Ö—É."""
        self.model.train()
        epoch_loss = 0.0
        total_samples = 0

        train_bar = tqdm(train_loader, desc="üîÑ –û–±—É—á–µ–Ω–∏–µ", colour="blue", leave=False)

        for batch in train_bar:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã batch –¥–∞–Ω–Ω—ã—Ö
            if isinstance(batch, tuple):
                batch_x, _ = batch  # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            else:
                batch_x = batch

            batch_x = batch_x.to(self.device)

            self.optimizer.zero_grad()
            reconstruction, _ = self.model(batch_x)

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            batch_size, seq_len, vocab_size = reconstruction.shape
            reconstruction_flat = reconstruction.view(-1, vocab_size)
            target_flat = batch_x.view(-1)
            loss = self.criterion(reconstruction_flat, target_flat)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            epoch_loss += loss.item() * batch_x.size(0)
            total_samples += batch_x.size(0)

            train_bar.set_postfix(
                {
                    "loss": f"{epoch_loss/total_samples:.4f}",
                    "lr": f"{self.optimizer.param_groups[0]['lr']:.2e}",
                }
            )

        return epoch_loss / total_samples

    def validate_epoch(self, val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è"):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—à–∏–±–æ–∫."""
        self.model.eval()
        val_errors = []

        val_bar = tqdm(val_loader, desc=f"üìä {desc}", leave=False, colour="green")

        with torch.no_grad():
            for batch in val_bar:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã batch –¥–∞–Ω–Ω—ã—Ö
                if isinstance(batch, tuple):
                    batch_x, _ = batch  # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                else:
                    batch_x = batch

                batch_x = batch_x.to(self.device)
                errors = self.model.get_reconstruction_error(batch_x)
                val_errors.extend(errors.cpu().numpy())

                val_bar.set_postfix(
                    {"–æ–±—Ä–∞–∑—Ü–æ–≤": len(val_errors), "—Å—Ä–µ–¥–Ω.–æ—à–∏–±–∫–∞": f"{np.mean(val_errors):.4f}"}
                )

        if len(val_errors) > 0:
            return {
                "mean_error": np.mean(val_errors),
                "std_error": np.std(val_errors),
                "median_error": np.median(val_errors),
                "samples_processed": len(val_errors),
                "errors": np.array(val_errors),
            }
        else:
            return {
                "mean_error": 0.0,
                "std_error": 0.0,
                "median_error": 0.0,
                "samples_processed": 0,
                "errors": np.array([]),
            }

    def train_autoencoder(
        self,
        train_loader,
        val_loader,
        epochs=25,
        patience=8,
        validate_every_n_epochs=1,
        threshold_percentile=95,
        model_name="autoencoder",
        output_dir="trials/autoencoder",
    ):
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.

        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
            epochs (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            patience (int): –¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            validate_every_n_epochs (int): –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            threshold_percentile (int): –ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ—Ä–æ–≥–∞
            model_name (str): –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–∞ {epochs} —ç–ø–æ—Ö")
        print(f"   üìä –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ {validate_every_n_epochs} —ç–ø–æ—Ö(–∏)")
        print(f"   ‚è∞ –¢–µ—Ä–ø–µ–Ω–∏–µ: {patience} —ç–ø–æ—Ö")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        model_dir = Path(output_dir)
        model_dir.mkdir(parents=True, exist_ok=True)

        best_val_error = float("inf")
        patience_counter = 0
        start_time = time.time()

        for epoch in range(1, epochs + 1):
            epoch_start = time.time()

            # –û–ë–£–ß–ï–ù–ò–ï
            epoch_loss = self.train_epoch(train_loader)
            self.history["train_losses"].append(epoch_loss)

            # –í–ê–õ–ò–î–ê–¶–ò–Ø
            val_stats = None
            if epoch % validate_every_n_epochs == 0:
                val_stats = self.validate_epoch(val_loader, f"–í–∞–ª–∏–¥–∞—Ü–∏—è —ç–ø–æ—Ö–∏ {epoch}")
                self.history["val_mean_errors"].append(val_stats["mean_error"])
                self.history["val_std_errors"].append(val_stats["std_error"])
                self.history["val_median_errors"].append(val_stats["median_error"])
                self.history["val_samples"].append(val_stats["samples_processed"])
                self.history["epochs_with_validation"].append(epoch)

            epoch_time = time.time() - epoch_start

            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —ç–ø–æ—Ö–∏
            if val_stats:
                print(
                    f"üìä –≠–ø–æ—Ö–∞ {epoch:2d}/{epochs} | "
                    f"Loss: {epoch_loss:.4f} | "
                    f"Val Œº: {val_stats['mean_error']:.4f} | "
                    f"Val œÉ: {val_stats['std_error']:.4f} | "
                    f"Val median: {val_stats['median_error']:.4f} | "
                    f"Samples: {val_stats['samples_processed']} | "
                    f"–í—Ä–µ–º—è: {epoch_time:.1f}—Å"
                )

                # Early stopping
                if val_stats["mean_error"] < best_val_error:
                    best_val_error = val_stats["mean_error"]
                    patience_counter = 0

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    torch.save(
                        {
                            "model_state_dict": self.model.state_dict(),
                            "epoch": epoch,
                            "train_loss": epoch_loss,
                            "val_error": val_stats["mean_error"],
                            "val_std": val_stats["std_error"],
                        },
                        model_dir / f"{model_name}_best.pth",
                    )
                    print(f"   üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å! Val error: {best_val_error:.4f}")

                else:
                    patience_counter += validate_every_n_epochs
                    print(f"   ‚è≥ Patience: {patience_counter}/{patience}")
                    if patience_counter >= patience:
                        print(f"\n‚è∞ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                        break
            else:
                print(
                    f"üìä –≠–ø–æ—Ö–∞ {epoch:2d}/{epochs} | "
                    f"Loss: {epoch_loss:.4f} | "
                    f"–í—Ä–µ–º—è: {epoch_time:.1f}—Å"
                )

        training_time = time.time() - start_time
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time/60:.1f} –º–∏–Ω—É—Ç")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        checkpoint = torch.load(model_dir / f"{model_name}_best.pth", weights_only=False)
        self.model.load_state_dict(checkpoint["model_state_dict"])

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüéØ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞...")
        val_stats_final = self.validate_epoch(val_loader, "–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥")
        threshold = np.percentile(val_stats_final["errors"], threshold_percentile)
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.4f} (–ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å {threshold_percentile}%)")

        return {
            "model_dir": model_dir,
            "model_name": model_name,
            "threshold": threshold,
            "threshold_percentile": threshold_percentile,
            "training_time": training_time,
            "training_history": self.history,
            "best_val_error": best_val_error,
            "stopped_epoch": checkpoint["epoch"],
            "final_val_stats": val_stats_final,
        }

    def test_autoencoder(self, test_loader, test_labels, threshold, results_dict=None):
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ + –∞—Ç–∞–∫–∏).

        Args:
            test_loader: DataLoader —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            test_labels: –ú–µ—Ç–∫–∏ (0=–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ, 1=–∞—Ç–∞–∫–∏)
            threshold (float): –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            results_dict (dict, optional): –°–ª–æ–≤–∞—Ä—å –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞...")

        self.model.eval()
        test_errors = []

        test_bar = tqdm(test_loader, desc="üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", colour="red")
        with torch.no_grad():
            for batch in test_bar:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã batch –¥–∞–Ω–Ω—ã—Ö
                if isinstance(batch, tuple):
                    batch_x, _ = batch  # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                else:
                    batch_x = batch

                batch_x = batch_x.to(self.device)
                errors = self.model.get_reconstruction_error(batch_x)
                test_errors.extend(errors.cpu().numpy())
                test_bar.set_postfix({"–æ–±—Ä–∞–∑—Ü–æ–≤": len(test_errors)})

        test_errors = np.array(test_errors)
        test_labels_np = test_labels.numpy() if hasattr(test_labels, "numpy") else test_labels

        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—à–∏–±–∫–∏ –ø–æ —Ç–∏–ø–∞–º
        normal_errors = test_errors[test_labels_np == 0]
        attack_errors = test_errors[test_labels_np == 1]

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        evaluation = evaluate_anomaly_detection(
            normal_errors,
            attack_errors,
            threshold_percentile=(
                int((threshold / max(test_errors)) * 100) if max(test_errors) > 0 else 95
            ),
        )

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print_metrics_report(evaluation, "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        separation = np.mean(attack_errors) - np.mean(normal_errors)
        print(f"\nüìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(
            f"   üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_errors)} ({len(normal_errors)} –Ω–æ—Ä–º, {len(attack_errors)} –∞—Ç–∞–∫)"
        )
        print(f"   üìè –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {separation:.4f}")
        print(f"   üìè –ü–æ—Ä–æ–≥: {threshold:.4f}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            "test_errors": test_errors,
            "test_labels": test_labels_np,
            "threshold": threshold,
            "evaluation": evaluation,
            "separation": separation,
            "error_statistics": {
                "normal_mean": np.mean(normal_errors),
                "normal_std": np.std(normal_errors),
                "attack_mean": np.mean(attack_errors),
                "attack_std": np.std(attack_errors),
            },
        }

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã
        if results_dict:
            results.update(results_dict)

        return results

    def save_results(self, results, output_path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª."""
        with open(output_path, "wb") as f:
            pickle.dump(results, f)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
