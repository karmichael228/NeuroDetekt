#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π

–°–æ–¥–µ—Ä–∂–∏—Ç:
- Trainer: –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- LSTMTrainer: —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è LSTM –º–æ–¥–µ–ª–µ–π
"""

import time
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from ..utils.helpers import EarlyStopping, LossTracker, TimeTracker, format_time
from ..utils.metrics import calculate_accuracy


class Trainer:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""

    def __init__(self, model, optimizer, criterion, device="cuda", log_dir="logs"):
        """
        Args:
            model: –ú–æ–¥–µ–ª—å PyTorch
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            criterion: –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            log_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ª–æ–≥–æ–≤
        """
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = torch.device(
            device if torch.cuda.is_available() and device == "cuda" else "cpu"
        )

        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.model = self.model.to(self.device)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–∫–µ—Ä—ã
        self.time_tracker = TimeTracker()
        self.loss_tracker = LossTracker(log_dir)
        self.early_stopping = None

        print(f"‚úÖ –¢—Ä–µ–Ω–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")

    def setup_early_stopping(self, patience=7, delta=0, path="best_model.pt", verbose=True):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É."""
        self.early_stopping = EarlyStopping(
            patience=patience, verbose=verbose, delta=delta, path=path
        )

    def train_epoch(self, train_loader):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–Ω—É —ç–ø–æ—Ö—É. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö."""
        raise NotImplementedError("–ú–µ—Ç–æ–¥ train_epoch –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω")

    def validate_epoch(self, val_loader):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–Ω—É —ç–ø–æ—Ö—É. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö."""
        raise NotImplementedError("–ú–µ—Ç–æ–¥ validate_epoch –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω")

    def train(self, train_loader, val_loader, epochs=10, verbose=True):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è.

        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            epochs (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            verbose (bool): –í—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å

        Returns:
            dict: –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        print(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {epochs} —ç–ø–æ—Ö")
        print(f"   üìä –û–±—É—á–∞—é—â–∏—Ö –±–∞—Ç—á–µ–π: {len(train_loader)}")
        print(f"   üîç –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π: {len(val_loader)}")

        start_time = time.time()

        for epoch in range(1, epochs + 1):
            self.time_tracker.start_epoch()

            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(train_loader)
            print(
                f"üìö –≠–ø–æ—Ö–∞ {epoch:3d}/{epochs} | –û–ë–£–ß–ï–ù–ò–ï   | Loss: {train_loss:.4f} | Accuracy: {train_acc:.3f}"
            )

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate_epoch(val_loader)
            print(
                f"üìä –≠–ø–æ—Ö–∞ {epoch:3d}/{epochs} | –í–ê–õ–ò–î–ê–¶–ò–Ø  | Loss: {val_loss:.4f} | Accuracy: {val_acc:.3f}",
                end="",
            )

            # –í—Ä–µ–º—è —ç–ø–æ—Ö–∏
            epoch_time = self.time_tracker.end_epoch()

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫–µ—Ä—ã
            self.loss_tracker.update(epoch, train_loss, val_loss, train_acc, val_acc, epoch_time)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–Ω–Ω—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏
            model_saved = False
            if self.early_stopping:
                prev_best_score = self.early_stopping.best_score
                self.early_stopping(val_loss, self.model)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–∏–ª–∞—Å—å –ª–∏ –º–æ–¥–µ–ª—å
                if self.early_stopping.best_score != prev_best_score:
                    model_saved = True
                    print(" | üíæ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
                else:
                    print(
                        f" | ‚è≥ Patience: {self.early_stopping.counter}/{self.early_stopping.patience}"
                    )

                if self.early_stopping.early_stop:
                    print(f"\n‚è∞ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                    break
            else:
                print("")  # –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç early stopping

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Ä–µ–º–µ–Ω–∏ —ç–ø–æ—Ö–∏
            if verbose:
                remaining_time = self.time_tracker.estimate_remaining(epoch, epochs)
                print(
                    f"‚è±Ô∏è  –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {format_time(epoch_time)} | –û—Å—Ç–∞–ª–æ—Å—å: {format_time(remaining_time)}"
                )
                print()  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —ç–ø–æ—Ö

        total_time = time.time() - start_time

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏ –≤—ã–≤–æ–¥–∏–º —Ä–µ–∑—é–º–µ
        history = self.loss_tracker.save_history()
        best_metrics = self.loss_tracker.get_best_metrics()

        from ..utils.helpers import print_training_summary

        print_training_summary(total_time, best_metrics)

        return history


class LSTMTrainer(Trainer):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è LSTM –º–æ–¥–µ–ª–µ–π."""

    def __init__(
        self, model, optimizer, criterion, device="cuda", log_dir="logs", gradient_clip=1.0
    ):
        """
        Args:
            gradient_clip (float): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è –∫–ª–∏–ø–ø–∏–Ω–≥–∞
        """
        super().__init__(model, optimizer, criterion, device, log_dir)
        self.gradient_clip = gradient_clip

        print(f"üß† LSTM —Ç—Ä–µ–Ω–µ—Ä –≥–æ—Ç–æ–≤ (gradient_clip={gradient_clip})")

    def train_epoch(self, train_loader):
        """–û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å –æ–¥–Ω—É —ç–ø–æ—Ö—É."""
        self.model.train()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        pbar = tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ", leave=False, colour="blue")

        for batch in pbar:
            # batch - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ (inputs, targets) –æ—Ç collate_fn
            if isinstance(batch, tuple):
                inputs, targets = batch
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
            else:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - batch - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä
                batch = batch.to(self.device)
                inputs = batch[:, :-1]  # –í—Å–µ —Ç–æ–∫–µ–Ω—ã –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                targets = batch[:, 1:]  # –í—Å–µ —Ç–æ–∫–µ–Ω—ã –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ

            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            self.optimizer.zero_grad()

            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = self.model(inputs)

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            loss = self.criterion(
                outputs.contiguous().view(-1, outputs.size(-1)), targets.contiguous().view(-1)
            )

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()

            # –ö–ª–∏–ø–ø–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            if self.gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
            self.optimizer.step()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_loss += loss.item()
            total_accuracy += calculate_accuracy(outputs, targets)
            num_batches += 1

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
            pbar.set_postfix(
                {
                    "loss": f"{total_loss/num_batches:.4f}",
                    "acc": f"{total_accuracy/num_batches:.3f}",
                }
            )

        return total_loss / num_batches, total_accuracy / num_batches

    def validate_epoch(self, val_loader):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç LSTM –º–æ–¥–µ–ª—å –æ–¥–Ω—É —ç–ø–æ—Ö—É."""
        self.model.eval()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0

        with torch.no_grad():
            pbar = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è", leave=False, colour="green")

            for batch in pbar:
                # batch - —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ (inputs, targets) –æ—Ç collate_fn
                if isinstance(batch, tuple):
                    inputs, targets = batch
                    inputs = inputs.to(self.device)
                    targets = targets.to(self.device)
                else:
                    # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - batch - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä
                    batch = batch.to(self.device)
                    inputs = batch[:, :-1]
                    targets = batch[:, 1:]

                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                outputs = self.model(inputs)

                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
                loss = self.criterion(
                    outputs.contiguous().view(-1, outputs.size(-1)), targets.contiguous().view(-1)
                )

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                total_accuracy += calculate_accuracy(outputs, targets)
                num_batches += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
                pbar.set_postfix(
                    {
                        "loss": f"{total_loss/num_batches:.4f}",
                        "acc": f"{total_accuracy/num_batches:.3f}",
                    }
                )

        return total_loss / num_batches, total_accuracy / num_batches
