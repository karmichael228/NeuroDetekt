#!/usr/bin/env python3
# -*- coding: utf-8 -*

"""
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç CLI –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ LSTM –¥–ª—è —Å–∏—Å—Ç–µ–º—ã NeuroDetekt c –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.
"""

import argparse
import time
import os
from datetime import datetime
from pathlib import Path

import torch
import numpy as np
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ç–∫–µ–Ω–¥ –±–µ–∑ GUI, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ Qt
import matplotlib.pyplot as plt
from torch.nn import functional as F
from torch.optim import Adam, lr_scheduler
from tqdm import tqdm
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from data_processing import get_data, load_data_splits, SequencePairDataset, SequenceDataset, collate_fn, test_collate_fn
from models import create_lstm_model
from training_utils import TimeTracker, LossTracker, validate_model

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


def create_parser():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description=f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ PLAID —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PyTorch.",
    )
    parser.add_argument(
        "--batch_size",
        default=128,
        type=int,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.",
    )
    parser.add_argument(
        "--epochs", default=15, type=int, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è."
    )
    parser.add_argument(
        "--early_stopping",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É –æ–±—É—á–µ–Ω–∏—è.",
    )
    parser.add_argument(
        "--cells",
        default=128,
        type=int,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫ –≤ LSTM —Å–ª–æ—è—Ö.",
    )
    parser.add_argument(
        "--depth",
        default=2,
        type=int,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM.",
    )
    parser.add_argument(
        "--dropout",
        default=0.3,
        type=float,
        help="–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç dropout.",
    )
    parser.add_argument(
        "--ratio",
        default=1,
        type=int,
        help="–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∏ –∞—Ç–∞–∫—É—é—â–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ",
    )
    parser.add_argument(
        "--patience",
        default=10,
        type=int,
        help="–¢–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è.",
    )
    parser.add_argument(
        "--trial",
        default=0,
        type=int,
        help="–ù–æ–º–µ—Ä –∏—Å–ø—ã—Ç–∞–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø—É—Ç–∏.",
    )
    parser.add_argument(
        "--device",
        default="cuda",
        choices=["cuda", "cpu"],
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (cuda –∏–ª–∏ cpu).",
    )
    parser.add_argument(
        "--num_workers",
        default=4,
        type=int,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.",
    )
    parser.add_argument(
        "--learning_rate",
        default=1e-3,
        type=float,
        help="–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.",
    )
    parser.add_argument(
        "--cv_folds",
        default=5,
        type=int,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏.",
    )
    parser.add_argument(
        "--use_cv",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.",
    )

    return parser


def get_device(device_arg):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (CUDA –∏–ª–∏ CPU)."""
    if device_arg == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        memory_allocated = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {gpu_name}")
        print(f"üìä –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {memory_allocated:.2f} GB")
    else:
        device = torch.device("cpu")
        print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    return device


def train_epoch(model, train_loader, optimizer, criterion, device, epoch, scheduler=None):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    epoch_start = time.time()
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –∫—ç—à CUDA –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    progress_bar = tqdm(train_loader, desc=f"üîÑ –≠–ø–æ—Ö–∞ {epoch}", 
                        bar_format="{l_bar}{bar:30}{r_bar}", 
                        ncols=100)
    
    for batch_idx, (inputs, targets) in enumerate(progress_bar):
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–∏ –≤ —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª
        targets = targets.view(-1)
        outputs = outputs.view(-1, outputs.size(2))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è padding
        non_pad_mask = (targets != 0)
        if non_pad_mask.sum() > 0:
            outputs = outputs[non_pad_mask]
            targets = targets[non_pad_mask]
            
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            progress_bar.set_postfix({
                "‚ùå –ø–æ—Ç–µ—Ä—è": f"{loss.item():.4f}", 
                "‚úÖ —Ç–æ—á–Ω–æ—Å—Ç—å": f"{100.0 * correct / total:.2f}%",
                "‚è±Ô∏è –≤—Ä–µ–º—è": f"{time.time() - epoch_start:.1f}s"
            })
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π
            if batch_idx % 50 == 0 and batch_idx > 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    # –®–∞–≥ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –Ω–µ –¥–µ–ª–∞–µ–º —Ç—É—Ç, —Ç–∞–∫ –∫–∞–∫ –¥–ª—è ReduceLROnPlateau –Ω—É–∂–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    epoch_loss = running_loss / total if total > 0 else 0
    epoch_acc = correct / total if total > 0 else 0
    
    epoch_time = time.time() - epoch_start
    print(f"\nüìà –ò—Ç–æ–≥–∏ —ç–ø–æ—Ö–∏ {epoch}:")
    print(f"   ‚ùå –ü–æ—Ç–µ—Ä—è: {epoch_loss:.4f}, ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {epoch_acc:.4f}")
    print(f"   ‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {epoch_time:.2f}s\n")
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –∫—ç—à CUDA –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return epoch_loss, epoch_acc, epoch_time


def create_fold_dataloaders(train_data, val_data, batch_size, num_workers):
    """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = SequencePairDataset(train_data)
    val_dataset = SequencePairDataset(val_data)
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=min(batch_size, len(train_dataset)),
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=min(batch_size, len(val_dataset)),
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader


def train_with_cv(train_data, test_data, test_labels, model_args, training_args):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    device = training_args['device']
    batch_size = training_args['batch_size']
    epochs = training_args['epochs']
    num_workers = training_args['num_workers']
    learning_rate = training_args['learning_rate']
    early_stopping = training_args['early_stopping']
    patience = training_args['patience']
    n_folds = training_args['cv_folds']
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    trials_dir = Path("trials")
    trials_dir.mkdir(exist_ok=True, parents=True)
    model_dir = trials_dir / f"lstm_plaid_cv_{n_folds}"
    model_dir.mkdir(exist_ok=True, parents=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –æ–¥–∏–Ω —Ä–∞–∑
    test_dataset = SequenceDataset(test_data)
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=test_collate_fn,
        num_workers=num_workers,
        pin_memory=(str(device) == 'cuda')
    )
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ñ–æ–ª–¥–∞–º
    fold_metrics = []
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ
    for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):
        print(f"\n{'='*40}")
        print(f"üîÑ –§–æ–ª–¥ {fold+1}/{n_folds}")
        print(f"{'='*40}")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
        fold_train_data = [train_data[i] for i in train_idx]
        fold_val_data = [train_data[i] for i in val_idx]
        
        print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(fold_train_data)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        print(f"üìä –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(fold_val_data)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        train_loader, val_loader = create_fold_dataloaders(
            fold_train_data, fold_val_data, batch_size, num_workers
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model, optimizer, criterion = create_lstm_model(
            vocab_size=model_args['vocab_size'],
            cells=model_args['cells'],
            depth=model_args['depth'],
            dropout=model_args['dropout'],
            device=device,
            learning_rate=learning_rate
        )
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
        scheduler = lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=3, verbose=True
        )
        
        # –î–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        best_val_loss = float("inf")
        best_model = None
        patience_counter = 0
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        train_losses = []
        val_losses = []
        train_accs = []
        val_accs = []
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        for epoch in range(1, epochs + 1):
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc, _ = train_epoch(
                model, train_loader, optimizer, criterion, device, epoch
            )
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = validate_epoch(model, val_loader, criterion, device, epoch)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
            scheduler.step(val_loss)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            train_accs.append(train_acc)
            val_accs.append(val_acc)
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            if early_stopping:
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"‚ö†Ô∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                        break
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if early_stopping and best_model is not None:
            model.load_state_dict(best_model)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        plt.figure(figsize=(10, 5))
        epochs_range = range(1, len(train_losses) + 1)
        
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, train_losses, 'b-', label='–û–±—É—á–µ–Ω–∏–µ')
        plt.plot(epochs_range, val_losses, 'r-', label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
        plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
        plt.xlabel('–≠–ø–æ—Ö–∏')
        plt.ylabel('–ü–æ—Ç–µ—Ä—è')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, train_accs, 'b-', label='–û–±—É—á–µ–Ω–∏–µ')
        plt.plot(epochs_range, val_accs, 'r-', label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–≠–ø–æ—Ö–∏')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.savefig(model_dir / f"fold_{fold+1}_metrics.png")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = model_dir / f"model_fold_{fold+1}.pt"
        torch.save(model, model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
        metrics = validate_model(model, test_loader, test_labels, device)
        fold_metrics.append(metrics)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–æ–ª–¥–∞ {fold+1} –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:")
        print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
        print(f"   üìè Precision: {metrics['precision']:.4f}")
        print(f"   üìè Recall: {metrics['recall']:.4f}")
        print(f"   üìè F1-–º–µ—Ä–∞: {metrics['f1_score']:.4f}")
        print(f"   üìè AUC-ROC: {metrics['roc_auc']:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        with open(model_dir / f"fold_{fold+1}_results.txt", "w") as f:
            f.write(f"–¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}\n")
            f.write(f"Precision: {metrics['precision']:.4f}\n")
            f.write(f"Recall: {metrics['recall']:.4f}\n")
            f.write(f"F1-–º–µ—Ä–∞: {metrics['f1_score']:.4f}\n")
            f.write(f"AUC-ROC: {metrics['roc_auc']:.4f}\n")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º
    avg_metrics = {
        'accuracy': np.mean([m['accuracy'] for m in fold_metrics]),
        'precision': np.mean([m['precision'] for m in fold_metrics]),
        'recall': np.mean([m['recall'] for m in fold_metrics]),
        'f1_score': np.mean([m['f1_score'] for m in fold_metrics]),
        'roc_auc': np.mean([m['roc_auc'] for m in fold_metrics])
    }
    
    print(f"\n{'='*40}")
    print(f"üìä –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º:")
    print(f"{'='*40}")
    print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {avg_metrics['accuracy']:.4f}")
    print(f"   üìè Precision: {avg_metrics['precision']:.4f}")
    print(f"   üìè Recall: {avg_metrics['recall']:.4f}")
    print(f"   üìè F1-–º–µ—Ä–∞: {avg_metrics['f1_score']:.4f}")
    print(f"   üìè AUC-ROC: {avg_metrics['roc_auc']:.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open(model_dir / "avg_results.txt", "w") as f:
        f.write(f"–¢–æ—á–Ω–æ—Å—Ç—å: {avg_metrics['accuracy']:.4f}\n")
        f.write(f"Precision: {avg_metrics['precision']:.4f}\n")
        f.write(f"Recall: {avg_metrics['recall']:.4f}\n")
        f.write(f"F1-–º–µ—Ä–∞: {avg_metrics['f1_score']:.4f}\n")
        f.write(f"AUC-ROC: {avg_metrics['roc_auc']:.4f}\n")
    
    return model_dir, avg_metrics


def validate_epoch(model, val_loader, criterion, device, epoch):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏."""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    progress_bar = tqdm(val_loader, desc=f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —ç–ø–æ—Ö–∞ {epoch}", 
                       bar_format="{l_bar}{bar:30}{r_bar}", 
                       ncols=100)
    
    with torch.no_grad():
        for inputs, targets in progress_bar:
            inputs, targets = inputs.to(device), targets.to(device)
            
            outputs = model(inputs)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–∏ –≤ —Ç–µ–Ω–∑–æ—Ä –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª
            targets = targets.view(-1)
            outputs = outputs.view(-1, outputs.size(2))
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è padding
            non_pad_mask = (targets != 0)
            if non_pad_mask.sum() > 0:
                outputs = outputs[non_pad_mask]
                targets = targets[non_pad_mask]
                
                loss = criterion(outputs, targets)
                
                running_loss += loss.item() * inputs.size(0)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                progress_bar.set_postfix({
                    "‚ùå –ø–æ—Ç–µ—Ä—è": f"{loss.item():.4f}", 
                    "‚úÖ —Ç–æ—á–Ω–æ—Å—Ç—å": f"{100.0 * correct / total:.2f}%"
                })
    
    epoch_loss = running_loss / total if total > 0 else 0
    epoch_acc = correct / total if total > 0 else 0
    
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏—è: ‚ùå –ü–æ—Ç–µ—Ä—è: {epoch_loss:.4f}, ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {epoch_acc:.4f}")
    
    return epoch_loss, epoch_acc


def validate_model(model, test_loader, test_labels, device):
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏."""
    model.eval()
    all_losses = []
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –∫—ç—à CUDA –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # –ü–æ–ª—É—á–∞–µ–º loss –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    with torch.no_grad():
        for sequences in tqdm(test_loader, desc="üìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ"):
            sequences = sequences.to(device)
            batch_size = sequences.size(0)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            outputs = model(sequences[:, :-1])
            targets = sequences[:, 1:]
            
            # –í—ã—á–∏—Å–ª—è–µ–º loss –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
            for i in range(batch_size):
                output = outputs[i].view(-1, outputs.size(-1))
                target = targets[i].view(-1)
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º padding
                mask = (target != 0)
                if mask.sum() > 0:
                    output = output[mask]
                    target = target[mask]
                    
                    loss = torch.nn.functional.nll_loss(output, target, reduction='mean')
                    all_losses.append(loss.item())
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø–æ—Ç–µ—Ä–∏ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–π (—á–µ–º –≤—ã—à–µ loss, —Ç–µ–º –≤—ã—à–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏)
    anomaly_scores = np.array(all_losses)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫–∏ [0, 1]
    if len(anomaly_scores) > 0 and max(anomaly_scores) > min(anomaly_scores):
        anomaly_scores = (anomaly_scores - min(anomaly_scores)) / (max(anomaly_scores) - min(anomaly_scores))
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
    normal_scores = anomaly_scores[:len(test_labels) - test_labels.sum().int().item()]
    attack_scores = anomaly_scores[len(test_labels) - test_labels.sum().int().item():]
    
    # –í—ã–≤–æ–¥–∏–º –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–æ–∫ –∞–Ω–æ–º–∞–ª–∏–π:")
    print(f"   üîπ –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –º–∏–Ω={normal_scores.min():.4f}, –º–∞–∫—Å={normal_scores.max():.4f}, —Å—Ä–µ–¥–Ω–µ–µ={normal_scores.mean():.4f}")
    print(f"   üîπ –ê—Ç–∞–∫—É—é—â–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –º–∏–Ω={attack_scores.min():.4f}, –º–∞–∫—Å={attack_scores.max():.4f}, —Å—Ä–µ–¥–Ω–µ–µ={attack_scores.mean():.4f}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ F1-–º–µ—Ä—ã
    best_threshold = 0.5
    best_f1 = 0
    
    for threshold in np.arange(0.05, 0.95, 0.05):
        y_pred = (anomaly_scores > threshold).astype(int)
        f1 = f1_score(test_labels, y_pred)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"   üîπ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.4f}")
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ª—É—á—à–∏–º –ø–æ—Ä–æ–≥–æ–º
    predictions = (anomaly_scores > best_threshold).astype(int)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'accuracy': accuracy_score(test_labels, predictions),
        'precision': precision_score(test_labels, predictions),
        'recall': recall_score(test_labels, predictions),
        'f1_score': f1_score(test_labels, predictions),
        'roc_auc': roc_auc_score(test_labels, anomaly_scores),
        'threshold': best_threshold,
        'normal_mean': float(normal_scores.mean()),
        'attack_mean': float(attack_scores.mean()),
        'score_diff': float(attack_scores.mean() - normal_scores.mean())
    }
    
    return metrics


def main(
    batch_size=128,
    epochs=15,
    early_stopping=False,
    cells=128,
    depth=2,
    dropout=0.3,
    ratio=1,
    patience=10,
    trial=0,
    device="cuda",
    num_workers=4,
    learning_rate=1e-3,
    cv_folds=5,
    use_cv=True,
):
    """–í—Ö–æ–¥–Ω–∞—è —Ç–æ—á–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    start_time = time.time()
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device = get_device(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    train_data, _, test_val, atk = load_data_splits(
        "plaid", train_pct=1.0, ratio=ratio
    )
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–µ—Ç–∫–∏
    test_data = test_val + atk
    test_labels = torch.zeros(len(test_val) + len(atk))
    test_labels[len(test_val):] = 1
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüîç –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(train_data)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(test_data)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
    print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ: {test_labels.sum().item()} –∞–Ω–æ–º–∞–ª–∏–π, {len(test_labels) - test_labels.sum().item()} –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    model_args = {
        'vocab_size': 229,  # –î–ª—è PLAID –¥–∞—Ç–∞—Å–µ—Ç–∞
        'cells': cells,
        'depth': depth,
        'dropout': dropout,
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = {
        'device': device,
        'batch_size': batch_size,
        'epochs': epochs,
        'early_stopping': early_stopping,
        'patience': patience,
        'num_workers': num_workers,
        'learning_rate': learning_rate,
        'cv_folds': cv_folds,
    }
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    if use_cv:
        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {cv_folds}-–∫—Ä–∞—Ç–Ω–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π")
        model_dir, metrics = train_with_cv(train_data, test_data, test_labels, model_args, training_args)
    else:
        # –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ–±—ã—á–Ω–æ
        print("\nüîÑ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏
        train_loader, _, (test_loader, test_labels) = get_data(
            "plaid", batch_size=batch_size, ratio=ratio, num_workers=num_workers
        )
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        trials_dir = Path("trials")
        trials_dir.mkdir(exist_ok=True, parents=True)
        model_dir = trials_dir / f"lstm_plaid_{epochs}"
        model_dir.mkdir(exist_ok=True, parents=True)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model, optimizer, criterion = create_lstm_model(
            vocab_size=model_args['vocab_size'],
            cells=model_args['cells'],
            depth=model_args['depth'],
            dropout=model_args['dropout'],
            device=device,
            learning_rate=learning_rate
        )
        
        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è (StepLR –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ç—Ä–∏–∫)
        scheduler = lr_scheduler.StepLR(
            optimizer, step_size=2, gamma=0.5
        )
        
        # –î–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        best_train_loss = float("inf")
        best_model = None
        patience_counter = 0
        
        # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        train_losses = []
        train_accs = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        for epoch in range(1, epochs + 1):
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc, epoch_time = train_epoch(
                model, train_loader, optimizer, criterion, device, epoch
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
            scheduler.step()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–µ
            if epoch == 1 and train_acc > 0.95:
                print("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–µ!")
                print("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.")
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
            if early_stopping:
                if train_loss < best_train_loss:
                    best_train_loss = train_loss
                    best_model = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"‚ö†Ô∏è –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                        break
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –±—ã–ª–∞ —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if early_stopping and best_model is not None:
            model.load_state_dict(best_model)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        plt.figure(figsize=(10, 5))
        epochs_range = range(1, len(train_losses) + 1)
        
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, train_losses, 'b-', label='–ü–æ—Ç–µ—Ä—è –æ–±—É—á–µ–Ω–∏—è')
        plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å')
        plt.xlabel('–≠–ø–æ—Ö–∏')
        plt.ylabel('–ü–æ—Ç–µ—Ä—è')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, train_accs, 'b-', label='–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–≠–ø–æ—Ö–∏')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.savefig(model_dir / "training_progress.png")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = model_dir / f"model_lstm_{trial:02d}.pt"
        torch.save(model, model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
        metrics = validate_model(model, test_loader, test_labels, device)
        
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:")
        print(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}")
        print(f"   üìè Precision: {metrics['precision']:.4f}")
        print(f"   üìè Recall: {metrics['recall']:.4f}")
        print(f"   üìè F1-–º–µ—Ä–∞: {metrics['f1_score']:.4f}")
        print(f"   üìè AUC-ROC: {metrics['roc_auc']:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        with open(model_dir / "test_results.txt", "w") as f:
            f.write(f"–¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.4f}\n")
            f.write(f"Precision: {metrics['precision']:.4f}\n")
            f.write(f"Recall: {metrics['recall']:.4f}\n")
            f.write(f"F1-–º–µ—Ä–∞: {metrics['f1_score']:.4f}\n")
            f.write(f"AUC-ROC: {metrics['roc_auc']:.4f}\n")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    total_time = time.time() - start_time
    hours, remainder = divmod(total_time, 3600)
    minutes, seconds = divmod(remainder, 60)
    
    print(f"\n‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å")
    print(f"üíæ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {model_dir}")


if __name__ == "__main__":
    parser = create_parser()
    args = parser.parse_args()
    main(**vars(args))
