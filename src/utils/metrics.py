#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹

Ğ¡Ğ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº:
- Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (accuracy)
- Precision, Recall, F1-score
- ROC AUC Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
"""

import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


def calculate_accuracy(outputs, targets):
    """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.
    
    Args:
        outputs (torch.Tensor): Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ [batch_size, seq_len, vocab_size]
        targets (torch.Tensor): Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€ [batch_size, seq_len]
        
    Returns:
        float: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ´Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹)
    """
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ²
    _, pred = outputs.max(dim=2)
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ padding (Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹)
    non_pad_mask = (targets != 0)
    
    # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ non-padding ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    correct = pred.eq(targets).masked_select(non_pad_mask).sum().item()
    total = non_pad_mask.sum().item()
    
    return correct / total if total > 0 else 0.0


def calculate_metrics(y_true, y_pred, y_scores=None):
    """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸.
    
    Args:
        y_true (array-like): Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ (0/1)
        y_pred (array-like): ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ (0/1)
        y_scores (array-like, optional): ĞÑ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ROC AUC
        
    Returns:
        dict: Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸
    """
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1_score': f1_score(y_true, y_pred, zero_division=0)
    }
    
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ROC AUC ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹
    if y_scores is not None:
        try:
            metrics['roc_auc'] = roc_auc_score(y_true, y_scores)
        except ValueError:
            metrics['roc_auc'] = 0.0
    
    return metrics


def find_optimal_threshold(y_true, y_scores, metric='f1'):
    """ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸.
    
    Args:
        y_true (array-like): Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸
        y_scores (array-like): ĞÑ†ĞµĞ½ĞºĞ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        metric (str): ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ('f1', 'precision', 'recall')
        
    Returns:
        tuple: (Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹_Ğ¿Ğ¾Ñ€Ğ¾Ğ³, Ğ»ÑƒÑ‡ÑˆĞµĞµ_Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ_Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸)
    """
    best_threshold = 0.5
    best_score = 0.0
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸
    for threshold in np.arange(0.05, 0.95, 0.05):
        y_pred = (y_scores > threshold).astype(int)
        
        if metric == 'f1':
            score = f1_score(y_true, y_pred, zero_division=0)
        elif metric == 'precision':
            score = precision_score(y_true, y_pred, zero_division=0)
        elif metric == 'recall':
            score = recall_score(y_true, y_pred, zero_division=0)
        else:
            raise ValueError(f"ĞĞµĞ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°: {metric}")
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    return best_threshold, best_score


def evaluate_anomaly_detection(normal_scores, attack_scores, threshold_percentile=95):
    """ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.
    
    Args:
        normal_scores (array-like): ĞÑ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        attack_scores (array-like): ĞÑ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ñ‚Ğ°Ğº
        threshold_percentile (int): ĞŸĞµÑ€Ñ†ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒ Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°
        
    Returns:
        dict: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸
    """
    # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    threshold = np.percentile(normal_scores, threshold_percentile)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
    all_scores = np.concatenate([normal_scores, attack_scores])
    all_labels = np.concatenate([
        np.zeros(len(normal_scores)),  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ = 0
        np.ones(len(attack_scores))    # ĞÑ‚Ğ°ĞºĞ¸ = 1
    ])
    all_predictions = (all_scores > threshold).astype(int)
    
    # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    metrics = calculate_metrics(all_labels, all_predictions, all_scores)
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    separation = np.mean(attack_scores) - np.mean(normal_scores)
    
    return {
        'threshold': threshold,
        'threshold_percentile': threshold_percentile,
        'metrics': metrics,
        'separation': separation,
        'normal_stats': {
            'mean': np.mean(normal_scores),
            'std': np.std(normal_scores),
            'median': np.median(normal_scores)
        },
        'attack_stats': {
            'mean': np.mean(attack_scores),
            'std': np.std(attack_scores),
            'median': np.median(attack_scores)
        }
    }


def print_metrics_report(metrics, title="ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾ Ğ¾Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.
    
    Args:
        metrics (dict): Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸
        title (str): Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {title}")
    print('='*60)
    
    if isinstance(metrics, dict) and 'metrics' in metrics:
        # Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¾Ñ‚ evaluate_anomaly_detection
        main_metrics = metrics['metrics']
        print(f"ğŸ¯ ĞŸĞ¾Ñ€Ğ¾Ğ³: {metrics['threshold']:.4f} "
              f"({metrics['threshold_percentile']}% Ğ¿ĞµÑ€Ñ†ĞµĞ½Ñ‚Ğ¸Ğ»ÑŒ)")
        print(f"ğŸ“ Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ: {metrics['separation']:.4f}")
        print(f"\nğŸ“ˆ ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸:")
        
        for metric, value in main_metrics.items():
            emoji = {'accuracy': 'ğŸ¯', 'precision': 'ğŸ”', 'recall': 'ğŸ“Š', 
                    'f1_score': 'ğŸ†', 'roc_auc': 'ğŸ“ˆ'}.get(metric, 'ğŸ“Š')
            print(f"   {emoji} {metric.title()}: {value:.4f} ({value*100:.2f}%)")
        
        print(f"\nğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº:")
        normal = metrics['normal_stats']
        attack = metrics['attack_stats']
        print(f"   ğŸŸ¢ ĞĞ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ: Î¼={normal['mean']:.4f}, Ïƒ={normal['std']:.4f}")
        print(f"   ğŸ”´ ĞÑ‚Ğ°ĞºĞ¸: Î¼={attack['mean']:.4f}, Ïƒ={attack['std']:.4f}")
        
    else:
        # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
        for metric, value in metrics.items():
            emoji = {'accuracy': 'ğŸ¯', 'precision': 'ğŸ”', 'recall': 'ğŸ“Š', 
                    'f1_score': 'ğŸ†', 'roc_auc': 'ğŸ“ˆ'}.get(metric, 'ğŸ“Š')
            print(f"   {emoji} {metric.title()}: {value:.4f} ({value*100:.2f}%)")
    
    print('='*60) 