#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ NeuroDetekt - –ì–ª–∞–≤–Ω—ã–π –º–æ–¥—É–ª—å —Å–∏—Å—Ç–µ–º—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ç–æ—Ä–∂–µ–Ω–∏–π

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:
- –ú–æ–¥–µ–ª–∏ (LSTM, GRU-Autoencoder)
- –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python main.py --mode train --model lstm --config config.yaml
    python main.py --mode test --model autoencoder --checkpoint model.pth
    python main.py --mode compare --models lstm,autoencoder
"""

import argparse
import yaml
from pathlib import Path
import torch
import sys
import time
import os
import numpy as np
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π —Å–∏—Å—Ç–µ–º—ã
from .models import LSTMModel, GRUAutoEncoder, create_lstm_model
from .training import LSTMTrainer, AutoencoderTrainer
from .testing import LSTMEvaluator, AutoencoderEvaluator
from .visualization import TrainingPlotter, ResultsPlotter, ComparisonPlotter
from .utils import get_data, load_data_splits, calculate_metrics, print_metrics_report


class NeuroDetekt:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã NeuroDetekt."""
    
    def __init__(self, config_path=None):
        """
        Args:
            config_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"üß† NeuroDetekt –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"   üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_path or '–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é'}")
    
    def load_config(self, config_path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        default_config = {
            'data': {
                'dataset': 'plaid',
                'batch_size': 64,
                'sequence_length': 100
            },
            'lstm': {
                'vocab_size': 229,
                'embedding_dim': 128,
                'hidden_size': 128,
                'num_layers': 2,
                'dropout': 0.25,
                'learning_rate': 1e-4
            },
            'autoencoder': {
                'vocab_size': 229,
                'embedding_dim': 128,
                'hidden_size': 128,
                'num_layers': 2,
                'latent_dim': 64,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
                'dropout': 0.2,
                'learning_rate': 5e-5
            },
            'training': {
                'epochs': 15,
                'patience': 8,
                'gradient_clip': 1.0
            },
            'paths': {
                'data_dir': 'data',
                'base_dir': 'trials',  # –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
                'logs_dir': 'logs',
                'plots_dir': 'plots'
            },
            'testing': {
                'auto_test': True,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
                'save_predictions': True,
                'plot_results': True
            }
        }
        
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self._merge_configs(default_config, user_config)
        
        return default_config
    
    def _merge_configs(self, default, user):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        for key, value in user.items():
            if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                self._merge_configs(default[key], value)
            else:
                default[key] = value
    
    def _create_experiment_dir(self, experiment_name):
        """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
        experiment_dir = Path(self.config['paths']['base_dir']) / experiment_name
        experiment_dir.mkdir(exist_ok=True, parents=True)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        self.current_experiment_dir = experiment_dir
        self.current_model_path = experiment_dir / f"{experiment_name}_best.pth"
        self.current_final_path = experiment_dir / f"{experiment_name}_final.pth"
        
        return experiment_dir
    
    def train_lstm(self, experiment_name="lstm_model", auto_test=None):
        """–û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å."""
        print(f"\nüß† –û–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_dir = self._create_experiment_dir(experiment_name)
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {experiment_dir}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        train_loader, val_loader, (test_loader, test_labels) = get_data(
            self.config['data']['dataset'],
            batch_size=self.config['data']['batch_size']
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model, optimizer, criterion = create_lstm_model(
            vocab_size=self.config['lstm']['vocab_size'],
            embedding_dim=self.config['lstm']['embedding_dim'],
            hidden_size=self.config['lstm']['hidden_size'],
            num_layers=self.config['lstm']['num_layers'],
            dropout=self.config['lstm']['dropout'],
            device=str(self.device),
            learning_rate=self.config['lstm']['learning_rate']
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer = LSTMTrainer(
            model, optimizer, criterion,
            device=str(self.device),
            log_dir=str(experiment_dir),
            gradient_clip=self.config['training']['gradient_clip']
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É
        trainer.setup_early_stopping(
            patience=self.config['training']['patience'],
            path=str(self.current_model_path)
        )
        
        # –û–±—É—á–∞–µ–º
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ '{experiment_name}'...")
        start_time = time.time()
        
        history = trainer.train(
            train_loader, val_loader,
            epochs=self.config['training']['epochs']
        )
        
        training_time = time.time() - start_time
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        torch.save(model.state_dict(), self.current_final_path)
        print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   Best: {self.current_model_path}")
        print(f"   Final: {self.current_final_path}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É
        print(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–æ—Ç—Ç–µ—Ä–∞...")
        try:
            plotter = TrainingPlotter(str(experiment_dir))
            print(f"üîß –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")
            training_plot = plotter.plot_training_history(history, f"LSTM –æ–±—É—á–µ–Ω–∏–µ - {experiment_name}", model_type='lstm')
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: {training_plot}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
            print(f"   (–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        import pickle
        history_path = experiment_dir / f"{experiment_name}_training_history.pkl"
        with open(history_path, 'wb') as f:
            pickle.dump(history, f)
        print(f"üìà –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è: {history_path}")
        
        # –°–æ–∑–¥–∞–µ–º summary —Ñ–∞–π–ª
        print(f"üîß –í—ã–∑–æ–≤ _save_training_summary...")
        try:
            self._save_training_summary(experiment_dir, experiment_name, history, training_time, 'LSTM')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ summary: {e}")
            import traceback
            traceback.print_exc()
        
        results = {
            'model': model,
            'history': history,
            'trainer': trainer,
            'experiment_dir': experiment_dir,
            'model_path': self.current_model_path,
            'training_time': training_time
        }
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        if auto_test is None:
            auto_test = self.config['testing'].get('auto_test', True)
        
        print(f"üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ auto_test: {auto_test}")
        if auto_test:
            print(f"\nüß™ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
            try:
                test_results = self._test_trained_model(
                    experiment_name, 'lstm', test_loader, test_labels
                )
                results['test_results'] = test_results
                print(f"‚úÖ –ê–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
                
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º summary —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                try:
                    self._save_training_summary(experiment_dir, experiment_name, history, training_time, 'LSTM', test_results)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ summary: {e}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"‚ö†Ô∏è –ê–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            # –°–æ–∑–¥–∞–µ–º summary –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            try:
                self._save_training_summary(experiment_dir, experiment_name, history, training_time, 'LSTM')
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ summary: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        print(f"\n{'='*60}")
        print(f"üéâ –û–ë–£–ß–ï–ù–ò–ï LSTM –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*60}")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {experiment_dir}")
        print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {self.current_model_path}")
        print(f"üìÑ Summary —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {experiment_dir / f'{experiment_name}_summary.txt'}")
        if 'test_results' in results:
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–µ–Ω—ã –≤ summary")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"{'='*60}")
        
        return results
    
    def _test_trained_model(self, experiment_name, model_type, test_loader, test_labels):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å."""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if model_type.lower() == 'lstm':
                model, _, _ = create_lstm_model(
                    vocab_size=self.config['lstm']['vocab_size'],
                    embedding_dim=self.config['lstm']['embedding_dim'],
                    hidden_size=self.config['lstm']['hidden_size'],
                    num_layers=self.config['lstm']['num_layers'],
                    dropout=self.config['lstm']['dropout'],
                    device=str(self.device),
                    learning_rate=self.config['lstm']['learning_rate']
                )
                model.load_state_dict(torch.load(self.current_model_path, map_location=self.device))
                model.eval()
                
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                from .utils.metrics import calculate_accuracy
                test_results = self._evaluate_lstm_on_test(
                    model, test_loader, test_labels, experiment_name
                )
            
            elif model_type.lower() in ['autoencoder', 'gru-autoencoder']:
                # –ê–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
                test_results = self._evaluate_autoencoder_on_test(
                    test_loader, test_labels, experiment_name
                )
            
            else:
                print(f"‚ö†Ô∏è –ê–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è {model_type} –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ")
                return None
                
            return test_results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _evaluate_lstm_on_test(self, model, test_loader, test_labels, experiment_name):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        model.eval()
        total_loss = 0.0
        total_accuracy = 0.0
        num_batches = 0
        all_outputs = []
        all_targets = []
        
        criterion = torch.nn.CrossEntropyLoss()
        
        print(f"üîç –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        with torch.no_grad():
            for i, batch in enumerate(test_loader):
                batch = batch.to(self.device)
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                inputs = batch[:, :-1]
                targets = batch[:, 1:]
                
                # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
                outputs = model(inputs)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
                loss = criterion(
                    outputs.contiguous().view(-1, outputs.size(-1)),
                    targets.contiguous().view(-1)
                )
                
                from .utils.metrics import calculate_accuracy
                accuracy = calculate_accuracy(outputs, targets)
                
                total_loss += loss.item()
                total_accuracy += accuracy
                num_batches += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                all_outputs.append(outputs.cpu())
                all_targets.append(targets.cpu())
        
        avg_loss = total_loss / num_batches
        avg_accuracy = total_accuracy / num_batches
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º LSTM –≤—ã—Ö–æ–¥—ã –≤ –æ—Ü–µ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏
        print(f"üî¨ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏...")
        anomaly_scores = []
        true_labels = test_labels.cpu().numpy()
        
        for batch_outputs, batch_targets in zip(all_outputs, all_targets):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –∫–∞–∫ –º–µ—Ä—É –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏
            batch_scores = []
            for seq_outputs, seq_targets in zip(batch_outputs, batch_targets):
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π cross-entropy –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                seq_loss = torch.nn.functional.cross_entropy(
                    seq_outputs.view(-1, seq_outputs.size(-1)),
                    seq_targets.view(-1),
                    reduction='mean'
                )
                batch_scores.append(seq_loss.item())
            anomaly_scores.extend(batch_scores)
        
        anomaly_scores = np.array(anomaly_scores)
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–Ω–∞–ª–∏–∑
        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞...")
        try:
            from .visualization import ResultsPlotter
            plotter = ResultsPlotter(str(self.current_experiment_dir))
            
            # ROC-AUC –∞–Ω–∞–ª–∏–∑
            roc_results = plotter.plot_roc_curve(
                true_labels, anomaly_scores,
                title=f"ROC-AUC –∞–Ω–∞–ª–∏–∑ - {experiment_name}",
                save_name=f"{experiment_name}_roc_curve.png"
            )
            
            # Precision-Recall –∞–Ω–∞–ª–∏–∑
            pr_results = plotter.plot_precision_recall_curve(
                true_labels, anomaly_scores,
                title=f"Precision-Recall –∞–Ω–∞–ª–∏–∑ - {experiment_name}",
                save_name=f"{experiment_name}_precision_recall.png"
            )
            
            # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            optimal_threshold = roc_results['optimal_threshold']
            comprehensive_results = plotter.create_comprehensive_analysis(
                true_labels, anomaly_scores, threshold=optimal_threshold,
                title=f"–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - {experiment_name}",
                save_name=f"{experiment_name}_comprehensive_analysis.png"
            )
            
            print(f"üìà –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            roc_results = {'auc': 0, 'optimal_threshold': 0.5}
            pr_results = {'average_precision': 0}
            comprehensive_results = {}
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_results = {
            'test_loss': avg_loss,
            'test_accuracy': avg_accuracy,
            'num_batches': num_batches,
            'total_samples': len(test_loader.dataset),
            'roc_auc': roc_results.get('auc', 0),
            'average_precision': pr_results.get('average_precision', 0),
            'optimal_threshold': roc_results.get('optimal_threshold', 0.5),
            'anomaly_scores': anomaly_scores,
            'true_labels': true_labels
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
        results_path = self.current_experiment_dir / f"{experiment_name}_test_results.txt"
        with open(results_path, 'w', encoding='utf-8') as f:
            f.write(f"üß™ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {experiment_name}\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"üìã –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\n")
            f.write(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: LSTM\n")
            f.write(f"Test Loss: {avg_loss:.4f}\n")
            f.write(f"Test Accuracy: {avg_accuracy:.3f} ({avg_accuracy*100:.1f}%)\n")
            f.write(f"–ë–∞—Ç—á–µ–π: {num_batches}\n")
            f.write(f"–û–±—Ä–∞–∑—Ü–æ–≤: {len(test_loader.dataset)}\n\n")
            
            f.write(f"üéØ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –ê–ù–û–ú–ê–õ–ò–ô:\n")
            f.write(f"ROC-AUC: {roc_results.get('auc', 0):.4f}\n")
            f.write(f"Average Precision: {pr_results.get('average_precision', 0):.4f}\n")
            f.write(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {roc_results.get('optimal_threshold', 0.5):.4f}\n\n")
            
            f.write(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–¶–ï–ù–û–ö:\n")
            normal_scores = anomaly_scores[true_labels == 0] 
            attack_scores = anomaly_scores[true_labels == 1]
            f.write(f"–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n")
            f.write(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {np.mean(normal_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(normal_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –°—Ç. –æ—Ç–∫–ª.: {np.std(normal_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(normal_scores)}\n\n")
            
            f.write(f"–ê—Ç–∞–∫–∏:\n")
            f.write(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {np.mean(attack_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(attack_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –°—Ç. –æ—Ç–∫–ª.: {np.std(attack_scores):.4f}\n")
            f.write(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(attack_scores)}\n\n")
            
            f.write(f"üí° –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:\n")
            f.write(f"–†–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö: {np.mean(attack_scores) - np.mean(normal_scores):.4f}\n")
            
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
            optimal_threshold = roc_results.get('optimal_threshold', 0.5)
            predictions = (anomaly_scores >= optimal_threshold).astype(int)
            
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            f.write(f"\nüéØ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø (–ø–æ—Ä–æ–≥ = {optimal_threshold:.4f}):\n")
            f.write(f"Accuracy: {accuracy_score(true_labels, predictions):.4f}\n")
            f.write(f"Precision: {precision_score(true_labels, predictions):.4f}\n")
            f.write(f"Recall: {recall_score(true_labels, predictions):.4f}\n")
            f.write(f"F1-Score: {f1_score(true_labels, predictions):.4f}\n\n")
            
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä
            total_normal = len(normal_scores)
            total_attacks = len(attack_scores)
            detected_attacks = np.sum(predictions[true_labels == 1])
            false_alarms = np.sum(predictions[true_labels == 0])
            
            f.write(f"üìà –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            f.write(f"–ò–∑ {total_attacks} –∞—Ç–∞–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {detected_attacks} ({detected_attacks/total_attacks*100:.1f}%)\n")
            f.write(f"–õ–æ–∂–Ω—ã—Ö —Ç—Ä–µ–≤–æ–≥ –∏–∑ {total_normal}: {false_alarms} ({false_alarms/total_normal*100:.1f}%)\n")
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
        print(f"   Test Loss: {avg_loss:.4f}")
        print(f"   Test Accuracy: {avg_accuracy:.3f} ({avg_accuracy*100:.1f}%)")
        print(f"   ROC-AUC: {roc_results.get('auc', 0):.4f}")
        print(f"   Average Precision: {pr_results.get('average_precision', 0):.4f}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        
        return test_results
    
    def _save_training_summary(self, experiment_dir, experiment_name, history, training_time, model_type, test_results=None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–¥–∫—É –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ."""
        summary_path = experiment_dir / f"{experiment_name}_summary.txt"
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
        current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–ª—é—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∏—Å—Ç–æ—Ä–∏–∏
        if 'val_loss' in history:
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç LSTM
            best_epoch = history['val_loss'].index(min(history['val_loss'])) + 1
            best_train_loss = history['train_loss'][best_epoch - 1]
            best_val_loss = history['val_loss'][best_epoch - 1]
            best_train_acc = history['train_acc'][best_epoch - 1]
            best_val_acc = history['val_acc'][best_epoch - 1]
            total_epochs = len(history['train_loss'])
        elif 'val_losses' in history:
            # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç LSTM
            best_epoch = history['val_losses'].index(min(history['val_losses'])) + 1
            best_train_loss = history['train_losses'][best_epoch - 1]
            best_val_loss = history['val_losses'][best_epoch - 1]
            best_train_acc = history['train_accs'][best_epoch - 1]
            best_val_acc = history['val_accs'][best_epoch - 1]
            total_epochs = len(history['train_losses'])
        elif 'val_mean_errors' in history:
            # –§–æ—Ä–º–∞—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
            if history['val_mean_errors']:
                best_epoch = history['val_mean_errors'].index(min(history['val_mean_errors'])) + 1
                best_train_loss = history['train_losses'][best_epoch - 1]
                best_val_error = history['val_mean_errors'][best_epoch - 1]
            else:
                best_epoch = len(history['train_losses'])
                best_train_loss = history['train_losses'][-1]
                best_val_error = 0
            total_epochs = len(history['train_losses'])
            # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–µ—Ç accuracy –º–µ—Ç—Ä–∏–∫
            best_train_acc = None
            best_val_acc = None
            best_val_loss = best_val_error  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—à–∏–±–∫—É –∫–∞–∫ loss
        else:
            print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è: {list(history.keys())}")
            return
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            f.write(f"–û–±—É—á–µ–Ω–∏–µ {model_type} –º–æ–¥–µ–ª–∏ '{experiment_name}' - {current_datetime}\n")
            f.write("=" * 60 + "\n\n")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            f.write("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n")
            if model_type.upper() == 'LSTM':
                f.write(f"  - –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.config['data']['batch_size']}\n")
                f.write(f"  - –≠–ø–æ—Ö–∏: {total_epochs}\n")
                f.write(f"  - Learning rate: {self.config['lstm']['learning_rate']}\n")
                f.write(f"  - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {self.config['lstm']['hidden_size']} hidden, {self.config['lstm']['num_layers']} layers, dropout {self.config['lstm']['dropout']}\n")
                f.write(f"  - –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}\n")
                f.write(f"  - Early stopping: –≤–∫–ª—é—á–µ–Ω–æ\n")
            elif model_type.upper() == 'GRU-AUTOENCODER':
                f.write(f"  - –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.config['data']['batch_size']}\n")
                f.write(f"  - –≠–ø–æ—Ö–∏: {total_epochs}\n")
                f.write(f"  - Learning rate: {self.config['autoencoder']['learning_rate']}\n")
                f.write(f"  - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {self.config['autoencoder']['hidden_size']} hidden, {self.config['autoencoder']['num_layers']} layers, dropout {self.config['autoencoder']['dropout']}\n")
                f.write(f"  - –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}\n")
                f.write(f"  - Early stopping: –≤–∫–ª—é—á–µ–Ω–æ\n")
            
            f.write(f"\n")
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if test_results:
                f.write("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n")
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
                if 'optimal_threshold' in test_results and 'anomaly_scores' in test_results and 'true_labels' in test_results:
                    threshold = test_results['optimal_threshold']
                    scores = test_results['anomaly_scores']
                    labels = test_results['true_labels']
                    
                    predictions = (scores >= threshold).astype(int)
                    
                    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
                    accuracy = accuracy_score(labels, predictions)
                    precision = precision_score(labels, predictions)
                    recall = recall_score(labels, predictions)
                    f1 = f1_score(labels, predictions)
                    
                    f.write(f"  - Accuracy: {accuracy:.4f}\n")
                    f.write(f"  - Precision: {precision:.4f}\n")
                    f.write(f"  - Recall: {recall:.4f}\n")
                    f.write(f"  - F1-–º–µ—Ä–∞: {f1:.4f}\n")
                    f.write(f"  - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.4f}\n")
                    
                    if 'roc_auc' in test_results:
                        f.write(f"  - ROC-AUC: {test_results['roc_auc']:.4f}\n")
                    if 'average_precision' in test_results:
                        f.write(f"  - Average Precision: {test_results['average_precision']:.4f}\n")
                        
                else:
                    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –Ω–µ—Ç
                    f.write(f"  - Test Loss: {test_results.get('test_loss', 'N/A')}\n")
                    f.write(f"  - Test Accuracy: {test_results.get('test_accuracy', 'N/A')}\n")
                    if 'roc_auc' in test_results:
                        f.write(f"  - ROC-AUC: {test_results['roc_auc']:.4f}\n")
            else:
                # –ï—Å–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
                f.write("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:\n")
                f.write(f"  - –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch}\n")
                f.write(f"  - Train Loss: {best_train_loss:.4f}\n")
                f.write(f"  - Val Loss: {best_val_loss:.4f}\n")
                
                # Accuracy –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è LSTM
                if best_train_acc is not None and best_val_acc is not None:
                    f.write(f"  - Train Accuracy: {best_train_acc:.4f}\n")
                    f.write(f"  - Val Accuracy: {best_val_acc:.4f}\n")
                elif model_type.upper() == 'GRU-AUTOENCODER':
                    # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
                    f.write(f"  - Val Mean Error: {best_val_loss:.4f}\n")
            
            f.write(f"\n–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥\n")
        
        print(f"üìÑ –°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {summary_path}")
    
    def train_autoencoder(self, experiment_name="autoencoder_model", auto_test=None):
        """–û–±—É—á–∞–µ—Ç GRU –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä."""
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ GRU –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_dir = self._create_experiment_dir(experiment_name)
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {experiment_dir}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞)
        train_loader, val_loader, (test_loader, test_labels) = get_data(
            self.config['data']['dataset'],
            batch_size=self.config['data']['batch_size'],
            normal_only=True  # –¢–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        from .models.gru_autoencoder import create_gru_autoencoder
        model, optimizer, criterion = create_gru_autoencoder(
            vocab_size=self.config['autoencoder']['vocab_size'],
            embedding_dim=self.config['autoencoder']['embedding_dim'],
            hidden_size=self.config['autoencoder']['hidden_size'],
            num_layers=self.config['autoencoder']['num_layers'],
            latent_dim=self.config['autoencoder']['latent_dim'],
            dropout=self.config['autoencoder']['dropout'],
            device=str(self.device),
            learning_rate=self.config['autoencoder']['learning_rate']
        )
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer = AutoencoderTrainer(
            model, optimizer, criterion,
            device=str(self.device),
            log_dir=str(experiment_dir)
        )
        
        # –û–±—É—á–∞–µ–º
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ '{experiment_name}'...")
        start_time = time.time()
        
        results = trainer.train_autoencoder(
            train_loader, val_loader,
            epochs=self.config['training']['epochs'],
            patience=self.config['training']['patience'],
            model_name=experiment_name,
            output_dir=str(experiment_dir)
        )
        
        training_time = time.time() - start_time
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
        plotter = TrainingPlotter(str(experiment_dir))
        training_plot = plotter.plot_training_history(
            results['training_history'], 
            f"–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –æ–±—É—á–µ–Ω–∏–µ - {experiment_name}",
            model_type='autoencoder'
        )
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: {training_plot}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è
        import pickle
        history_path = experiment_dir / f"{experiment_name}_training_history.pkl"
        with open(history_path, 'wb') as f:
            pickle.dump(results['training_history'], f)
        print(f"üìà –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è: {history_path}")
        
        experiment_results = {
            'model': model,
            'results': results,
            'trainer': trainer,
            'experiment_dir': experiment_dir,
            'training_time': training_time
        }
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        if auto_test is None:
            auto_test = self.config['testing'].get('auto_test', True)
            
        if auto_test:
            print(f"\nüß™ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞...")
            test_results = self._test_trained_model(
                experiment_name, 'autoencoder', test_loader, test_labels
            )
            experiment_results['test_results'] = test_results
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º summary —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            try:
                self._save_training_summary(experiment_dir, experiment_name, results['training_history'], training_time, 'GRU-Autoencoder', test_results)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ summary: {e}")
        else:
            print(f"‚ö†Ô∏è –ê–≤—Ç–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            # –°–æ–∑–¥–∞–µ–º summary –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            try:
                self._save_training_summary(experiment_dir, experiment_name, results['training_history'], training_time, 'GRU-Autoencoder')
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ summary: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        print(f"\n{'='*60}")
        print(f"üéâ –û–ë–£–ß–ï–ù–ò–ï GRU-AUTOENCODER –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*60}")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {experiment_dir}")
        print(f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {experiment_dir / f'{experiment_name}_best.pth'}")
        print(f"üìÑ Summary —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {experiment_dir / f'{experiment_name}_summary.txt'}")
        if 'test_results' in experiment_results:
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–µ–Ω—ã –≤ summary")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"{'='*60}")
        
        return experiment_results
    
    def test_model(self, model_path, model_type="lstm", experiment_name=None):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å."""
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_type.upper()} –º–æ–¥–µ–ª–∏...")
        
        # –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞–µ–º –∏–º—è –∏–∑ –ø—É—Ç–∏
        if experiment_name is None:
            experiment_name = f"test_{Path(model_path).stem}"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_dir = self._create_experiment_dir(f"test_{experiment_name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        _, _, (test_loader, test_labels) = get_data(
            self.config['data']['dataset'],
            batch_size=self.config['data']['batch_size']
        )
        
        if model_type.lower() == "lstm":
            # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º LSTM –º–æ–¥–µ–ª—å
            evaluator = LSTMEvaluator.load_from_checkpoint(
                model_path, LSTMModel,
                model_params={'vocab_size': self.config['lstm']['vocab_size']},
                device=str(self.device)
            )
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º (–Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏ –∞—Ç–∞–∫–∏)
            # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
            results = evaluator.evaluate_anomaly_detection(
                test_loader, test_loader  # –ó–∞–≥–ª—É—à–∫–∞
            )
            
        elif model_type.lower() == "autoencoder":
            # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
            evaluator = AutoencoderEvaluator.load_from_checkpoint(
                model_path, GRUAutoEncoder,
                model_params={'vocab_size': self.config['autoencoder']['vocab_size']},
                device=str(self.device)
            )
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º
            results = evaluator.evaluate_anomaly_detection(
                test_loader, test_loader  # –ó–∞–≥–ª—É—à–∫–∞
            )
        
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        plotter = ResultsPlotter(str(test_dir))
        if 'normal_scores' in results and 'attack_scores' in results:
            plotter.plot_score_distributions(
                results['normal_scores'], 
                results['attack_scores'],
                results.get('threshold'),
                f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã {model_type.upper()}"
            )
        
        return results
    
    def compare_models(self, model_specs, comparison_name="model_comparison"):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π.
        
        Args:
            model_specs: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (path, model_type) –∏–ª–∏ —Å—Ç—Ä–æ–∫ "type:path"
            comparison_name: –∏–º—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        print(f"\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_dir = self._create_experiment_dir(comparison_name)
        
        results = {}
        histories = {}
        
        # –ü–∞—Ä—Å–∏–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
        if isinstance(model_specs, str):
            model_specs = model_specs.split(',')
        
        parsed_specs = []
        for spec in model_specs:
            if isinstance(spec, tuple):
                path, model_type = spec
            else:
                if ':' in spec:
                    model_type, path = spec.split(':', 1)
                else:
                    raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {spec}")
            parsed_specs.append((path, model_type))
        
        for path, model_type in parsed_specs:
            print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_type}: {path}")
            model_results = self.test_model(path, model_type, f"{comparison_name}_{model_type}")
            results[f"{model_type}"] = model_results.get('evaluation', {}).get('metrics', {})
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
            history_path = Path(path).parent / "training_history.pkl"
            if history_path.exists():
                import pickle
                with open(history_path, 'rb') as f:
                    histories[model_type] = pickle.load(f)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏
        plotter = ComparisonPlotter(str(comparison_dir))
        
        if results:
            plotter.plot_metrics_comparison(results, "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π")
        
        if histories:
            plotter.plot_training_comparison(histories, "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è")
        
        return results
    
    def create_config_template(self, output_path="config_template.yaml"):
        """–°–æ–∑–¥–∞–µ—Ç —à–∞–±–ª–æ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        print(f"üìÑ –®–∞–±–ª–æ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω: {output_path}")

    def _evaluate_autoencoder_on_test(self, test_loader, test_labels, experiment_name):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        print(f"üîç –û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        from .models.gru_autoencoder import create_gru_autoencoder
        model, _, _ = create_gru_autoencoder(
            vocab_size=self.config['autoencoder']['vocab_size'],
            embedding_dim=self.config['autoencoder']['embedding_dim'],
            hidden_size=self.config['autoencoder']['hidden_size'],
            num_layers=self.config['autoencoder']['num_layers'],
            latent_dim=self.config['autoencoder']['latent_dim'],
            dropout=self.config['autoencoder']['dropout'],
            device=str(self.device),
            learning_rate=self.config['autoencoder']['learning_rate']
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        checkpoint = torch.load(self.current_model_path, map_location=self.device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        model.to(self.device)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä
        from .training import AutoencoderTrainer
        trainer = AutoencoderTrainer(model, None, None, device=str(self.device))
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä–æ–≥ –∏–∑ checkpoint
        threshold = checkpoint.get('val_error', 0) * 1.5  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –ø–æ—Ä–æ–≥
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if hasattr(self, 'current_experiment_dir'):
            # –ò—â–µ–º pickle —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
            import pickle
            try:
                history_path = self.current_experiment_dir / f"{experiment_name}_training_history.pkl"
                if history_path.exists():
                    with open(history_path, 'rb') as f:
                        history = pickle.load(f)
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –∫–∞–∫ 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
                    if 'val_mean_errors' in history and history['val_mean_errors']:
                        threshold = np.percentile(history['val_mean_errors'], 95)
            except:
                pass
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print(f"üî¨ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏...")
        all_errors = []
        
        model.eval()
        with torch.no_grad():
            for batch in test_loader:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã batch –¥–∞–Ω–Ω—ã—Ö
                if isinstance(batch, tuple):
                    batch_x, _ = batch
                else:
                    batch_x = batch
                
                batch_x = batch_x.to(self.device)
                errors = model.get_reconstruction_error(batch_x)
                all_errors.extend(errors.cpu().numpy())
        
        anomaly_scores = np.array(all_errors)
        true_labels = test_labels.cpu().numpy() if hasattr(test_labels, 'cpu') else test_labels
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—à–∏–±–∫–∏ –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö
        normal_errors = anomaly_scores[true_labels == 0]
        attack_errors = anomaly_scores[true_labels == 1]
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        from sklearn.metrics import roc_curve
        fpr, tpr, thresholds = roc_curve(true_labels, anomaly_scores)
        optimal_idx = np.argmax(tpr - fpr)
        optimal_threshold = thresholds[optimal_idx]
        
        print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –∞–Ω–∞–ª–∏–∑
        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞...")
        try:
            from .visualization import ResultsPlotter
            plotter = ResultsPlotter(str(self.current_experiment_dir))
            
            # ROC-AUC –∞–Ω–∞–ª–∏–∑
            roc_results = plotter.plot_roc_curve(
                true_labels, anomaly_scores,
                title=f"ROC-AUC –∞–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ - {experiment_name}",
                save_name=f"{experiment_name}_roc_curve.png"
            )
            
            # Precision-Recall –∞–Ω–∞–ª–∏–∑
            pr_results = plotter.plot_precision_recall_curve(
                true_labels, anomaly_scores,
                title=f"Precision-Recall –∞–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ - {experiment_name}",
                save_name=f"{experiment_name}_precision_recall.png"
            )
            
            # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            comprehensive_results = plotter.create_comprehensive_analysis(
                true_labels, anomaly_scores, threshold=optimal_threshold,
                title=f"–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ - {experiment_name}",
                save_name=f"{experiment_name}_comprehensive_analysis.png"
            )
            
            print(f"üìà –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            roc_results = {'auc': 0, 'optimal_threshold': optimal_threshold}
            pr_results = {'average_precision': 0}
            comprehensive_results = {}
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        test_results = {
            'reconstruction_threshold': threshold,
            'optimal_threshold': optimal_threshold,
            'total_samples': len(anomaly_scores),
            'normal_samples': len(normal_errors),
            'attack_samples': len(attack_errors),
            'roc_auc': roc_results.get('auc', 0),
            'average_precision': pr_results.get('average_precision', 0),
            'anomaly_scores': anomaly_scores,
            'true_labels': true_labels,
            'normal_mean_error': np.mean(normal_errors),
            'attack_mean_error': np.mean(attack_errors),
            'separation': np.mean(attack_errors) - np.mean(normal_errors)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
        results_path = self.current_experiment_dir / f"{experiment_name}_test_results.txt"
        with open(results_path, 'w', encoding='utf-8') as f:
            f.write(f"üß™ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ {experiment_name}\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"üìã –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\n")
            f.write(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: GRU-Autoencoder\n")
            f.write(f"–¢–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(anomaly_scores)}\n")
            f.write(f"–ù–æ—Ä–º–∞–ª—å–Ω—ã—Ö: {len(normal_errors)}\n")
            f.write(f"–ê—Ç–∞–∫: {len(attack_errors)}\n\n")
            
            f.write(f"üéØ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –ê–ù–û–ú–ê–õ–ò–ô:\n")
            f.write(f"ROC-AUC: {roc_results.get('auc', 0):.4f}\n")
            f.write(f"Average Precision: {pr_results.get('average_precision', 0):.4f}\n")
            f.write(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}\n\n")
            
            f.write(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–®–ò–ë–û–ö –†–ï–ö–û–ù–°–¢–†–£–ö–¶–ò–ò:\n")
            f.write(f"–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n")
            f.write(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {np.mean(normal_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(normal_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –°—Ç. –æ—Ç–∫–ª.: {np.std(normal_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(normal_errors)}\n\n")
            
            f.write(f"–ê—Ç–∞–∫–∏:\n")
            f.write(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {np.mean(attack_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {np.median(attack_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –°—Ç. –æ—Ç–∫–ª.: {np.std(attack_errors):.4f}\n")
            f.write(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(attack_errors)}\n\n")
            
            f.write(f"üí° –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:\n")
            f.write(f"–†–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö: {np.mean(attack_errors) - np.mean(normal_errors):.4f}\n")
            
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
            predictions = (anomaly_scores >= optimal_threshold).astype(int)
            
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            f.write(f"\nüéØ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø (–ø–æ—Ä–æ–≥ = {optimal_threshold:.4f}):\n")
            f.write(f"Accuracy: {accuracy_score(true_labels, predictions):.4f}\n")
            f.write(f"Precision: {precision_score(true_labels, predictions):.4f}\n")
            f.write(f"Recall: {recall_score(true_labels, predictions):.4f}\n")
            f.write(f"F1-Score: {f1_score(true_labels, predictions):.4f}\n\n")
            
            # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä
            detected_attacks = np.sum(predictions[true_labels == 1])
            false_alarms = np.sum(predictions[true_labels == 0])
            
            f.write(f"üìà –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")
            f.write(f"–ò–∑ {len(attack_errors)} –∞—Ç–∞–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {detected_attacks} ({detected_attacks/len(attack_errors)*100:.1f}%)\n")
            f.write(f"–õ–æ–∂–Ω—ã—Ö —Ç—Ä–µ–≤–æ–≥ –∏–∑ {len(normal_errors)}: {false_alarms} ({false_alarms/len(normal_errors)*100:.1f}%)\n")
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞:")
        print(f"   ROC-AUC: {roc_results.get('auc', 0):.4f}")
        print(f"   Average Precision: {pr_results.get('average_precision', 0):.4f}")
        print(f"   –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.mean(attack_errors) - np.mean(normal_errors):.4f}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        
        return test_results


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(
        description="NeuroDetekt - –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—Ç–æ—Ä–∂–µ–Ω–∏–π",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--mode", 
        choices=["train", "test", "compare", "config"],
        required=True,
        help="–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã"
    )
    
    parser.add_argument(
        "--model",
        choices=["lstm", "autoencoder"],
        help="–¢–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è/—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    
    parser.add_argument(
        "--config",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"
    )
    
    parser.add_argument(
        "--checkpoint",
        help="–ü—É—Ç—å –∫ checkpoint –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
    )
    
    parser.add_argument(
        "--models",
        help="–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)"
    )
    
    parser.add_argument(
        "--name",
        default="model",
        help="–ò–º—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
    parser.add_argument(
        "--hidden-size", 
        type=int, 
        help="–†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--num-layers", 
        type=int, 
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--dropout", 
        type=float, 
        help="Dropout rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--latent-dim", 
        type=int, 
        help="Latent dimension –¥–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--learning-rate", 
        type=float, 
        help="Learning rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    parser.add_argument(
        "--epochs", 
        type=int, 
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--batch-size", 
        type=int, 
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--patience", 
        type=int, 
        help="Patience –¥–ª—è early stopping (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)"
    )
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    system = NeuroDetekt(args.config)
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if args.hidden_size is not None:
        system.config['lstm']['hidden_size'] = args.hidden_size
        system.config['autoencoder']['hidden_size'] = args.hidden_size
        
    if args.num_layers is not None:
        system.config['lstm']['num_layers'] = args.num_layers
        system.config['autoencoder']['num_layers'] = args.num_layers
        
    if args.dropout is not None:
        system.config['lstm']['dropout'] = args.dropout
        system.config['autoencoder']['dropout'] = args.dropout
        
    if args.latent_dim is not None:
        system.config['autoencoder']['latent_dim'] = args.latent_dim
        
    if args.learning_rate is not None:
        system.config['lstm']['learning_rate'] = args.learning_rate
        system.config['autoencoder']['learning_rate'] = args.learning_rate
        
    if args.epochs is not None:
        system.config['training']['epochs'] = args.epochs
        
    if args.batch_size is not None:
        system.config['data']['batch_size'] = args.batch_size
        
    if args.patience is not None:
        system.config['training']['patience'] = args.patience
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for path in system.config['paths'].values():
        Path(path).mkdir(exist_ok=True, parents=True)
    
    # –í—ã–≤–æ–¥–∏–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if args.mode == "train":
        model_type = args.model or "unknown"
        print(f"\n‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è {model_type.upper()}:")
        if model_type == "lstm":
            print(f"   Hidden Size: {system.config['lstm']['hidden_size']}")
            print(f"   Layers: {system.config['lstm']['num_layers']}")
            print(f"   Dropout: {system.config['lstm']['dropout']}")
            print(f"   Learning Rate: {system.config['lstm']['learning_rate']}")
        elif model_type == "autoencoder":
            print(f"   Hidden Size: {system.config['autoencoder']['hidden_size']}")
            print(f"   Layers: {system.config['autoencoder']['num_layers']}")
            print(f"   Latent Dim: {system.config['autoencoder']['latent_dim']}")
            print(f"   Dropout: {system.config['autoencoder']['dropout']}")
            print(f"   Learning Rate: {system.config['autoencoder']['learning_rate']}")
        print(f"   Epochs: {system.config['training']['epochs']}")
        print(f"   Batch Size: {system.config['data']['batch_size']}")
        print(f"   Patience: {system.config['training']['patience']}")
    
    try:
        if args.mode == "train":
            if not args.model:
                print("‚ùå –î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏ (--model)")
                sys.exit(1)
            
            if args.model == "lstm":
                results = system.train_lstm(args.name)
                print(f"‚úÖ LSTM –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {args.name}")
                
            elif args.model == "autoencoder":
                results = system.train_autoencoder(args.name)
                print(f"‚úÖ –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ {args.name}")
        
        elif args.mode == "test":
            if not args.checkpoint:
                print("‚ùå –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (--checkpoint)")
                sys.exit(1)
            
            if not args.model:
                print("‚ùå –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏ (--model)")
                sys.exit(1)
            
            results = system.test_model(args.checkpoint, args.model)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞")
        
        elif args.mode == "compare":
            if not args.models:
                print("‚ùå –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª–∏ (--models)")
                sys.exit(1)
            
            results = system.compare_models(args.models)
            print(f"‚úÖ –ú–æ–¥–µ–ª–∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã")
        
        elif args.mode == "config":
            system.create_config_template()
            print("‚úÖ –®–∞–±–ª–æ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω")
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 