#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GRU-Autoencoder –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö.
–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
–í—ã—Å–æ–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∞–Ω–æ–º–∞–ª–∏—é.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
import numpy as np

__author__ = "KARMICHAEL228"
__license__ = "MIT"


class GRUAutoEncoder(nn.Module):
    """GRU-–∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö.
    
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. –≠–Ω–∫–æ–¥–µ—Ä: –≠–º–±–µ–¥–¥–∏–Ω–≥ + GRU -> –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    2. –î–µ–∫–æ–¥–µ—Ä: –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ -> GRU + –ø—Ä–æ–µ–∫—Ü–∏—è -> —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
    """
    
    def __init__(self, vocab_size=229, embedding_dim=128, hidden_size=128, 
                 num_layers=2, latent_dim=64, dropout=0.2):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç GRU-–∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä."""
        super(GRUAutoEncoder, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.latent_dim = latent_dim
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–π
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # –≠–Ω–∫–æ–¥–µ—Ä - GRU —Å–ª–æ–∏
        self.encoder_gru = nn.GRU(
            embedding_dim, 
            hidden_size, 
            num_layers=num_layers, 
            batch_first=True, 
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        self.to_latent = nn.Sequential(
            nn.Linear(hidden_size, latent_dim),
            nn.Tanh()
        )
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –∏–∑ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        self.from_latent = nn.Linear(latent_dim, hidden_size * num_layers)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –≤—Ö–æ–¥–æ–≤ –¥–µ–∫–æ–¥–µ—Ä–∞
        self.latent_to_input = nn.Linear(latent_dim, hidden_size)
        
        # –î–µ–∫–æ–¥–µ—Ä - GRU —Å–ª–æ–∏
        self.decoder_gru = nn.GRU(
            hidden_size,  # –í—Ö–æ–¥ - —ç—Ç–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output_layer = nn.Linear(hidden_size, vocab_size)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._init_weights()
    
    def _init_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏."""
        for name, param in self.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                param.data.fill_(0)
            elif 'embedding' in name:
                nn.init.uniform_(param.data, -0.1, 0.1)
    
    def encode(self, x):
        """–≠–Ω–∫–æ–¥–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ."""
        # –≠–º–±–µ–¥–¥–∏–Ω–≥
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
        embedded = self.dropout(embedded)
        
        # –≠–Ω–∫–æ–¥–µ—Ä GRU
        _, hidden = self.encoder_gru(embedded)  # hidden: [num_layers, batch_size, hidden_size]
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        last_hidden = hidden[-1]  # [batch_size, hidden_size]
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        latent = self.to_latent(last_hidden)  # [batch_size, latent_dim]
        
        return latent
    
    def decode(self, latent, seq_len):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å."""
        batch_size = latent.size(0)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞
        hidden_states = self.from_latent(latent)  # [batch_size, hidden_size * num_layers]
        hidden = hidden_states.view(batch_size, self.num_layers, self.hidden_size)  # [batch_size, num_layers, hidden_size]
        hidden = hidden.transpose(0, 1).contiguous()  # [num_layers, batch_size, hidden_size]
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥—ã –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞ - –ø–æ–≤—Ç–æ—Ä—è–µ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        repeated_latent = latent.unsqueeze(1).repeat(1, seq_len, 1)  # [batch_size, seq_len, latent_dim]
        
        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å hidden_size –¥–ª—è –≤—Ö–æ–¥–∞ –≤ GRU
        decoder_input = torch.tanh(self.latent_to_input(repeated_latent.view(-1, self.latent_dim)))  # [batch_size * seq_len, hidden_size]
        decoder_input = decoder_input.view(batch_size, seq_len, self.hidden_size)  # [batch_size, seq_len, hidden_size]
        
        # –î–µ–∫–æ–¥–µ—Ä GRU
        decoder_output, _ = self.decoder_gru(decoder_input, hidden)
        decoder_output = self.dropout(decoder_output)
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–ª–æ–≤–∞—Ä—è
        output = self.output_layer(decoder_output)  # [batch_size, seq_len, vocab_size]
        
        return output
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä."""
        seq_len = x.size(1)
        
        # –≠–Ω–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        latent = self.encode(x)
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        reconstruction = self.decode(latent, seq_len)
        
        return reconstruction, latent
    
    def get_reconstruction_error(self, x):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—à–∏–±–∫—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)."""
        self.eval()
        with torch.no_grad():
            reconstruction, _ = self.forward(x)
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            batch_size, seq_len = x.shape
            
            # –í—ã—á–∏—Å–ª—è–µ–º log_softmax –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
            log_probs = F.log_softmax(reconstruction, dim=-1)  # [batch_size, seq_len, vocab_size]
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è padding (—Ç–æ–∫–µ–Ω 0)
            padding_mask = (x != 0).float()  # [batch_size, seq_len]
            
            # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º gather –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω—É–∂–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            true_token_log_probs = log_probs.gather(2, x.unsqueeze(-1)).squeeze(-1)  # [batch_size, seq_len]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π log-likelihood
            masked_log_probs = true_token_log_probs * padding_mask  # –û–±–Ω—É–ª—è–µ–º padding
            negative_log_likelihood = -masked_log_probs.sum(dim=1)  # –°—É–º–º–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ [batch_size]
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            valid_token_counts = padding_mask.sum(dim=1)  # [batch_size]
            valid_token_counts = torch.clamp(valid_token_counts, min=1)  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
            
            normalized_errors = negative_log_likelihood / valid_token_counts
            
            return normalized_errors


def create_gru_autoencoder(vocab_size=229, embedding_dim=128, hidden_size=128, 
                          num_layers=2, latent_dim=64, dropout=0.2, 
                          device="cuda", learning_rate=0.00005):
    """–°–æ–∑–¥–∞–µ—Ç GRU-–∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.
    
    Args:
        vocab_size (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        embedding_dim (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è GRU
        num_layers (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ GRU
        latent_dim (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        dropout (float): –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥—Ä–æ–ø–∞—É—Ç–∞
        device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        learning_rate (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        tuple: (–º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å)
    """
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = GRUAutoEncoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        latent_dim=latent_dim,
        dropout=dropout
    )
    
    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model = model.to(device)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä–∞
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º padding
    
    return model, optimizer, criterion


class GRUAutoEncoderDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ GRU-–∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä–∞."""
    
    def __init__(self, model, threshold=None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä.
        
        Args:
            model (GRUAutoEncoder): –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–µ–Ω–∫–æ–¥–µ—Ä–∞
            threshold (float): –ü–æ—Ä–æ–≥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        """
        self.model = model
        self.threshold = threshold
    
    def fit_threshold(self, normal_data, percentile=95):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            normal_data (torch.utils.data.DataLoader): –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            percentile (float): –ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞
        """
        self.model.eval()
        errors = []
        
        with torch.no_grad():
            for batch_x, batch_y in normal_data:
                if torch.cuda.is_available():
                    batch_x = batch_x.cuda()
                
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã (label = 0)
                normal_mask = (batch_y == 0)
                if normal_mask.sum() > 0:
                    normal_x = batch_x[normal_mask]
                    batch_errors = self.model.get_reconstruction_error(normal_x)
                    errors.extend(batch_errors.cpu().numpy())
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –∫–∞–∫ percentile-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –æ—à–∏–±–æ–∫ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
 
        print(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Ä–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–∏: {self.threshold:.4f} (–ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å {percentile}%)")
        
        return self.threshold
    
    def predict(self, x):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏.
        
        Args:
            x (torch.Tensor): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            tuple: (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è [0/1], –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
        """
        if self.threshold is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Ä–æ–≥ —Å –ø–æ–º–æ—â—å—é fit_threshold()")
        
        errors = self.model.get_reconstruction_error(x)
        predictions = (errors > self.threshold).long()
        
        return predictions, errors
    
    def predict_proba(self, x):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª–∏–π.
        
        Args:
            x (torch.Tensor): –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            torch.Tensor: –û—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–±–æ–ª—å—à–µ = –±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–∞—è –∞–Ω–æ–º–∞–ª–∏—è)
        """
        return self.model.get_reconstruction_error(x) 