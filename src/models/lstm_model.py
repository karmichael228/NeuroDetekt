#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üß† LSTM –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Embedding ‚Üí LSTM ‚Üí Dropout ‚Üí Linear ‚Üí LogSoftmax
"""

import torch
import torch.nn as nn
from torch.optim import Adam

__author__ = "karmichael228"
__license__ = "MIT"


class LSTMModel(nn.Module):
    """LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤.
    
    Attributes:
        embedding: –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã
        lstm_layers: –°—Ç–µ–∫ LSTM —Å–ª–æ–µ–≤  
        dropout: Dropout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        output_layer: –í—ã—Ö–æ–¥–Ω–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        softmax: LogSoftmax –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    
    def __init__(self, vocab_size=229, embedding_dim=128, hidden_size=128, num_layers=2, dropout=0.25):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç LSTM –º–æ–¥–µ–ª—å.
        
        Args:
            vocab_size (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤)
            embedding_dim (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞  
            hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è LSTM
            num_layers (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM
            dropout (float): –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥—Ä–æ–ø–∞—É—Ç–∞
        """
        super(LSTMModel, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # –°–ª–æ–∏ –º–æ–¥–µ–ª–∏
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–∫ LSTM —Å–ª–æ–µ–≤
        self.lstm_layers = nn.ModuleList()
        for i in range(num_layers):
            input_size = embedding_dim if i == 0 else hidden_size
            self.lstm_layers.append(nn.LSTM(input_size, hidden_size, batch_first=True))
        
        self.dropout = nn.Dropout(dropout)
        self.output_layer = nn.Linear(hidden_size, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å.
        
        Args:
            x (torch.Tensor): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len]
            
        Returns:
            torch.Tensor: –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len, vocab_size]
        """
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        x = self.embedding(x)
        
        # –ü—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å—Ç–µ–∫ LSTM —Å–ª–æ–µ–≤
        for lstm_layer in self.lstm_layers:
            x, _ = lstm_layer(x)
            x = self.dropout(x)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π —Å–æ—Ñ—Ç–º–∞–∫—Å
        x = self.output_layer(x)
        x = self.softmax(x)
        
        return x
    
    def get_model_info(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_type': 'LSTM',
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'hidden_size': self.hidden_size,
            'num_layers': self.num_layers,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params
        }


def create_lstm_model(vocab_size=229, embedding_dim=128, hidden_size=128, num_layers=2, dropout=0.25, device="cuda", learning_rate=0.0001):
    """–°–æ–∑–¥–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º –∏ —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å.

    Args:
        vocab_size (int): –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
        embedding_dim (int): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        hidden_size (int): –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è LSTM
        num_layers (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM
        dropout (float): –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥—Ä–æ–ø–∞—É—Ç–∞
        device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda' –∏–ª–∏ 'cpu')
        learning_rate (float): –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        tuple: (–º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å)
    """
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = LSTMModel(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    
    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device_obj = torch.device(device if torch.cuda.is_available() and device == "cuda" else "cpu")
    model = model.to(device_obj)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
    optimizer = Adam(model.parameters(), lr=learning_rate)
    criterion = nn.NLLLoss()
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
    info = model.get_model_info()
    print(f"‚úÖ LSTM –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {info['total_parameters']:,}")
    print(f"   üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {info['embedding_dim']}‚Üí{info['hidden_size']}√ó{info['num_layers']}")
    print(f"   üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_obj}")
    
    return model, optimizer, criterion 